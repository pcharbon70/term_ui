# 6.12.4 ClusterDashboard Widget

## Problem Statement

Build a ClusterDashboard widget that visualizes distributed Erlang clusters with node status, health metrics, and cross-node process information. This is the fourth BEAM introspection widget that leverages Erlang's runtime capabilities for live system visualization in distributed environments.

### Impact

- Enables monitoring of distributed BEAM applications
- Provides visibility into cluster health and connectivity
- Helps detect and diagnose network partitions
- Shows process group membership across nodes

## Solution Overview

Implement a stateful widget that:
1. Displays connected nodes with status indicators (connected/disconnected)
2. Shows node health metrics (CPU, memory, scheduler load)
3. Displays cross-node process registry (global registered names, :pg groups)
4. Detects and alerts on network partitions
5. Shows connection/disconnection events
6. Provides RPC interface for remote node inspection

### Key Design Decisions

1. **Erlang Distribution APIs**: Use `:erlang.nodes()`, `:net_kernel`, `:global`, `:pg` for cluster info
2. **Node Health Metrics**: Use `:erlang.statistics/1` and `:erlang.memory/0` via RPC calls
3. **Event Detection**: Monitor `:net_kernel` events via `:net_kernel.monitor_nodes/2`
4. **Single-node Testing**: Design to work in non-distributed mode gracefully

## Technical Details

### Dependencies

- `TermUI.StatefulComponent` behavior
- `TermUI.Event` for keyboard handling
- `TermUI.Renderer.Style` for styling
- Erlang distribution APIs (`:erlang`, `:net_kernel`, `:global`, `:pg`, `:rpc`)

### Files to Create/Modify

1. `lib/term_ui/widgets/cluster_dashboard.ex` - Main widget implementation
2. `test/term_ui/widgets/cluster_dashboard_test.exs` - Unit tests
3. `examples/cluster_dashboard/` - Example application

### API Design

```elixir
# Create widget props
ClusterDashboard.new(
  update_interval: 2000,        # Refresh interval
  show_health_metrics: true,    # Show CPU/memory/load
  show_pg_groups: true,         # Show :pg process groups
  show_global_names: true,      # Show :global registered names
  on_node_select: &callback/1   # Selection callback
)

# Keyboard controls
# - Up/Down: Navigate node list
# - Enter: Toggle node details
# - r: Refresh now
# - g: Show :global names
# - p: Show :pg groups
# - i: Inspect selected node (RPC)
# - Escape: Close details
# - q: Quit (in example app)
```

## Implementation Plan

### Task 1: Core Widget Structure
- [x] Create `cluster_dashboard.ex` with StatefulComponent behavior
- [x] Implement `new/1` props function with configuration options
- [x] Implement `init/1` with state initialization
- [x] Implement `mount/1` to start refresh timer and node monitoring
- [x] Implement `unmount/1` for cleanup

### Task 2: Connected Nodes List (6.12.4.1)
- [x] Implement `fetch_nodes/0` to get connected nodes via `:erlang.nodes/0`
- [x] Build node info map with name, status, connection time
- [x] Handle local node display
- [x] Implement node list rendering with status indicators

### Task 3: Node Health Metrics (6.12.4.2)
- [x] Implement `fetch_node_metrics/1` using RPC to query remote nodes
- [x] Fetch scheduler utilization via `:erlang.statistics(:scheduler_wall_time)`
- [x] Fetch memory info via `:erlang.memory/0`
- [x] Fetch system load info
- [x] Handle RPC failures gracefully with timeout

### Task 4: Cross-node Process Registry (6.12.4.3)
- [x] Implement `fetch_global_names/0` via `:global.registered_names/0`
- [x] Show whereis info for global names
- [x] Display node where each global process runs

### Task 5: PG Group Membership (6.12.4.4)
- [x] Implement `fetch_pg_groups/0` via `:pg.which_groups/0`
- [x] Show members of each group via `:pg.get_members/1`
- [x] Display node distribution of group members

### Task 6: Network Partition Detection (6.12.4.5)
- [x] Track known nodes history
- [x] Detect when previously connected node disconnects
- [x] Alert on potential partition (multiple nodes disconnect simultaneously)
- [x] Style alerts prominently

### Task 7: Connection Events (6.12.4.6)
- [x] Use `:net_kernel.monitor_nodes/2` for node events
- [x] Track connection/disconnection timestamps
- [x] Show recent events in event log section
- [x] Implement event history with max size

### Task 8: RPC Interface (6.12.4.7)
- [x] Implement `rpc_call/4` wrapper with timeout handling
- [x] Provide node inspection (process count, memory, uptime)
- [x] Show remote node's connected nodes view
- [x] Handle `{:badrpc, _}` errors gracefully

### Task 9: Event Handling
- [x] Implement keyboard navigation (up/down, page up/down)
- [x] Implement enter for details toggle
- [x] Implement view mode switching (nodes, globals, pg groups)
- [x] Implement refresh trigger

### Task 10: Rendering
- [x] Render header with cluster summary
- [x] Render node list with status indicators
- [x] Render health metrics per node
- [x] Render details panel for selected node
- [x] Render events log
- [x] Render footer with keyboard hints

### Task 11: Unit Tests
- [x] Test widget initialization
- [x] Test event handling for navigation
- [x] Test view mode switching
- [x] Test rendering with mock data
- [x] Test local node (non-distributed) behavior

### Task 12: Example Application
- [x] Create example app structure
- [x] Implement demo with local node info
- [x] Add documentation for testing with multiple nodes

## Success Criteria

1. ✅ Widget displays connected nodes with status
2. ✅ Health metrics show CPU/memory/load per node
3. ✅ Global registered names are visible
4. ✅ PG group membership is displayed
5. ✅ Network partition events trigger alerts
6. ✅ Connection events are logged and displayed
7. ✅ RPC inspection works for remote nodes
8. ✅ Works gracefully in non-distributed mode
9. ✅ All tests pass (41 tests)
10. ✅ Example application demonstrates features

## Notes

- Testing distributed features requires starting multiple nodes
- Can test most functionality with single (local) node
- RPC calls have timeouts to prevent hangs
- Node monitoring requires calling `:net_kernel.monitor_nodes/2` early
